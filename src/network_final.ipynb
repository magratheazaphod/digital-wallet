{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Production version of code for Insight Data Engineers program.\n",
    "\n",
    "# Overall strategy - define module \"account\" which contains a list of \"contacts\"\n",
    "# Contacts are saved as Python dictionary, using account number as key and the \"tier\" level as value\n",
    "# \"tier\" being the shorthand for the number of degrees of separation.\n",
    "# Since Python dictionaries are extremely fast (equivalent to hash tables), should be able to flag transaction fast\n",
    "\n",
    "import pandas as pd\n",
    "import csv\n",
    "import numpy as np\n",
    "from user_class import User, find_new_friends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Paths to data files\n",
    "input_dir = '../paymo_input/'\n",
    "batch_file = 'batch_payment.csv'\n",
    "stream_file = 'stream_payment.csv'\n",
    "batch_path = input_dir + batch_file\n",
    "stream_path = input_dir + stream_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Difficult to parse messages section because it's possible for it to use commas\n",
    "## Below, figured out solution - import raw string, then use regular expressions to pick out each component\n",
    "## we choose everything after the fourth comma to be the message\n",
    "batch_raw = []\n",
    "\n",
    "with open(batch_path, newline='') as csvfile:\n",
    "    for line in csvfile:\n",
    "        batch_raw.append(line.strip())\n",
    "        \n",
    "df_batch = pd.DataFrame(batch_raw, columns = ['raw'])\n",
    "\n",
    "## USE REGEX to parse out - everything before first comma is time, second comma is id1 and so on\n",
    "## able to pick up messages with commas in them since it picks up all leftover characters at end of line\n",
    "df_batch['time'] = df_batch['raw'].str.extract('^(.+?),', expand=True)\n",
    "df_batch['id1'] = df_batch['raw'].str.extract('^[^,]*,([^,]*),', expand=True)\n",
    "df_batch['id2'] = df_batch['raw'].str.extract('^[^,]*,[^,]*,([^,]*),', expand=True)\n",
    "df_batch['amount'] = df_batch['raw'].str.extract('^[^,]*,[^,]*,[^,]*,([^,]*),', expand=True)\n",
    "df_batch['message'] = df_batch['raw'].str.extract('^[^,]*,[^,]*,[^,]*,[^,]*,(.*$)', expand=True)\n",
    "\n",
    "#can get rid of raw data now that we've parsed everything\n",
    "df_batch = df_batch.drop('raw', 1)\n",
    "df_batch = df_batch.drop(0, 0) #drop header row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#for convenience, let's call the id1 users 'givers' and id2 users 'receivers'\n",
    "#strategy is to use the pandas groupby command to obtain list of all partners in transactions where <user_id> is giver\n",
    "#repeat for transactions where <user_id> is receiver, then combine into single list\n",
    "givers = df_batch.groupby('id1')\n",
    "receivers = df_batch.groupby('id2')\n",
    "partners_1 = {}\n",
    "partners_2 = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping invalid key:  no. Even if the union were a matter of economic indifference\n",
      "Skipping invalid key:  and even if it were to be disadvantageous from the economic standpoint\n"
     ]
    }
   ],
   "source": [
    "## find all transactions where user <user_id> was giver, then find list of partners in those transactions\n",
    "for user_id,transactions in givers:\n",
    "    \n",
    "    #store list of all transaction partners as list (easiest type to extend later)\n",
    "    try:\n",
    "        partners_1[int(user_id)] = list(givers.get_group(user_id)['id2'].astype(int))\n",
    "   \n",
    "    #some lines of batch_payment.txt and stream_payment.txt are off - omit malformed entries\n",
    "    except (KeyError, ValueError) as BadLine:\n",
    "        print(\"Skipping invalid key:\",user_id)\n",
    "    \n",
    "## same as before for all transactions where <user_id> was receiver\n",
    "for user_id,transactions in receivers:\n",
    "    \n",
    "    #store list of all transaction partners as list (easiest type to extend later)\n",
    "    try: \n",
    "        partners_2[int(user_id)] = list(receivers.get_group(user_id)['id1'].astype(int))\n",
    "        \n",
    "    #some lines of batch_payment.txt and stream_payment.txt are off - omit malformed entries\n",
    "    except (KeyError, ValueError) as BadLine:\n",
    "        print(\"Skipping invalid key:\",user_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## it's possible that some users only show up as givers and others only as receivers - combine to master list of all IDs\n",
    "## in actuality for the provided batch_payment.txt all users show up as givers at least once, but not safe to assume\n",
    "user_list_1 = np.array(list(partners_1.keys()))\n",
    "user_list_2 = np.array(list(partners_2.keys()))\n",
    "user_list = np.unique(np.concatenate([user_list_1,user_list_2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "user_master_list = {}\n",
    "\n",
    "#cycle through all users and agglomerate partners from all transactions\n",
    "#conversion back and forth between list and numpy array is pretty fast\n",
    "#lists easier to append to, hence why stored as list, but also wanted to use numpy.unique function.\n",
    "for user_id in user_list:\n",
    "    \n",
    "    pp = []\n",
    "    \n",
    "    if user_id in partners_1.keys():\n",
    "        pp += partners_1[user_id]\n",
    "        \n",
    "    if user_id in partners_2.keys():\n",
    "        pp += partners_2[user_id]\n",
    "        \n",
    "    #reduce to (sorted) list of all unique partners\n",
    "    user_master_list[user_id] = User(user_id, list(np.unique(pp)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building lists of connections of degree 2 for each user...\n",
      "Done. Connections of degree n accessible via User.friends[n]\n"
     ]
    }
   ],
   "source": [
    "## use the find_new_friends function (stored in user_class.py) to supplement friend tiers down to level of interest\n",
    "#this is the most labor-intensive part of the program, unsurprisingly - n=2 takes 2 minutes on my macbook pro\n",
    "\n",
    "#unfortunately, n=3 and n=4 are highly memory intensive...couldn't get them to work consistently\n",
    "tier_depth = 2\n",
    "\n",
    "#successively add tiers of friendship to every user in user_master_list\n",
    "for tier in range(2,tier_depth+1):\n",
    "    \n",
    "    print(\"Building lists of connections of degree\", tier, \"for each user...\")\n",
    "    for user_id, user_info in user_master_list.items():\n",
    "        user_info.friends[tier] = find_new_friends(user_master_list,user_info,tier)\n",
    "        \n",
    "print(\"Done. Connections of degree n accessible via User.friends[n]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## create forward lookup ##\n",
    "## for each user, dictionary dos links a user id with the level of friendship\n",
    "for user_id, user_data in user_master_list.items():\n",
    "    user_data.build_dos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## DEFINE TESTS ##\n",
    "\n",
    "## the following tests take in pairs of user IDs as strings, convert them to int and use\n",
    "## User.dos to look up whether they are connected to each other, and if so at what degree.\n",
    "\n",
    "#first test - have these people had a transaction with each other in the batch data set?\n",
    "def test1(id1,id2):\n",
    "    \n",
    "    if type(id1)==str and type(id2)==str:\n",
    "    \n",
    "        #converts user IDs from string into ints, which we use as dictionary keys\n",
    "        try:\n",
    "            user1 = int(id1)\n",
    "        except ValueError:\n",
    "            return 'unverified'\n",
    "\n",
    "        try:\n",
    "            user2 = int(id2)\n",
    "        except ValueError:\n",
    "            return 'unverified'\n",
    "        \n",
    "        #this being python, the following line doesn't take up new memory - just a shorthand\n",
    "        if user1 in user_master_list.keys(): #have to check in case id1 is a new user\n",
    "            user_dos = user_master_list[user1].dos\n",
    "\n",
    "            if user2 in user_dos.keys():\n",
    "                if user_dos[user2] == 1:\n",
    "                    return 'trusted'\n",
    "    \n",
    "    return 'unverified'\n",
    "\n",
    "#second test - is the transaction partner either a 1st or 2nd degree connection?    \n",
    "def test2(id1,id2):\n",
    "    \n",
    "    if type(id1)==str and type(id2)==str:\n",
    "    \n",
    "        #converts user IDs from string into ints, which we use as dictionary keys\n",
    "        try:\n",
    "            user1 = int(id1)\n",
    "        except ValueError:\n",
    "            return 'unverified'\n",
    "\n",
    "        try:\n",
    "            user2 = int(id2)\n",
    "        except ValueError:\n",
    "            return 'unverified'\n",
    "    \n",
    "        #this being python, the following line doesn't take up new memory - just a shorthand\n",
    "        if user1 in user_master_list.keys(): #have to check in case id1 is a new user\n",
    "\n",
    "            user_dos = user_master_list[user1].dos\n",
    "\n",
    "            if user2 in user_dos.keys():\n",
    "                if user_dos[user2] in range (1,3):\n",
    "                    return 'trusted'\n",
    "    \n",
    "    return 'unverified'\n",
    "    \n",
    "#third test - is the transaction partner at least a 4th degree connection?    \n",
    "def test3(id1,id2):\n",
    "    \n",
    "    if type(id1)==str and type(id2)==str:\n",
    "    \n",
    "        #converts user IDs from string into ints, which we use as dictionary keys\n",
    "        try:\n",
    "            user1 = int(id1)\n",
    "        except ValueError:\n",
    "            return 'unverified'\n",
    "\n",
    "        try:\n",
    "            user2 = int(id2)\n",
    "        except ValueError:\n",
    "            return 'unverified'\n",
    "        \n",
    "        #this being python, the following line doesn't take up new memory - just a shorthand\n",
    "        if user1 in user_master_list.keys(): #have to check in case id1 is a new user    \n",
    "\n",
    "            user_dos = user_master_list[user1].dos\n",
    "\n",
    "            if user2 in user_dos.keys():\n",
    "                if user_dos[user2] in range(1,5):\n",
    "                    return 'trusted'\n",
    "\n",
    "    return 'unverified'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## again, using regular expressions to parse from raw data.\n",
    "stream_raw = []\n",
    "\n",
    "with open(stream_path, newline='') as csvfile:\n",
    "    for line in csvfile:\n",
    "        stream_raw.append(line.strip())\n",
    "        \n",
    "df_stream = pd.DataFrame(stream_raw, columns = ['raw'])\n",
    "\n",
    "## USE REGEX to parse out - everything before first comma is time, second comma is id1 and so on\n",
    "## able to pick up messages with commas in them since it picks up all leftover characters at end of line\n",
    "df_stream['time'] = df_stream['raw'].str.extract('^(.+?),', expand=True)\n",
    "df_stream['id1'] = df_stream['raw'].str.extract('^[^,]*,([^,]*),', expand=True)\n",
    "df_stream['id2'] = df_stream['raw'].str.extract('^[^,]*,[^,]*,([^,]*),', expand=True)\n",
    "df_stream['amount'] = df_stream['raw'].str.extract('^[^,]*,[^,]*,[^,]*,([^,]*),', expand=True)\n",
    "df_stream['message'] = df_stream['raw'].str.extract('^[^,]*,[^,]*,[^,]*,[^,]*,(.*$)', expand=True)\n",
    "\n",
    "#can get rid of raw data now that we've parsed everything\n",
    "df_stream = df_stream.drop('raw', 1)\n",
    "df_stream = df_stream.drop(0, 0) #drop header row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 1 results:\n",
      "unverified    1543084\n",
      "trusted       1456949\n",
      "Name: test1, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "## create output columns\n",
    "## I found that pandas started to slow down drastically when trying to calculate over 1,000,000\n",
    "## entries at a time...hence broken into pieces\n",
    "\n",
    "#test 1 - immediate adjacency\n",
    "test1_a = df_stream[:1000000].apply(lambda x: test1(x['id1'], x['id2']), axis=1)\n",
    "test1_b = df_stream[1000000:2000000].apply(lambda x: test1(x['id1'], x['id2']), axis=1)\n",
    "test1_c = df_stream[2000000:].apply(lambda x: test1(x['id1'], x['id2']), axis=1)\n",
    "df_stream['test1'] = pd.concat([test1_a, test1_b, test1_c])\n",
    "\n",
    "print('Test 1 results:')\n",
    "print(df_stream['test1'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 2 results:\n",
      "trusted       2347124\n",
      "unverified     652909\n",
      "Name: test2, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#test 2 - 2nd degree friends pass as verified\n",
    "test2_a = df_stream[:1000000].apply(lambda x: test2(x['id1'], x['id2']), axis=1)\n",
    "test2_b = df_stream[1000000:2000000].apply(lambda x: test2(x['id1'], x['id2']), axis=1)\n",
    "test2_c = df_stream[2000000:].apply(lambda x: test2(x['id1'], x['id2']), axis=1)\n",
    "df_stream['test2'] = pd.concat([test2_a, test2_b, test2_c])\n",
    "\n",
    "print('Test 2 results:')\n",
    "print(df_stream['test2'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 3 results:\n",
      "trusted       2347124\n",
      "unverified     652909\n",
      "Name: test3, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#test 3\n",
    "test3_a = df_stream[:1000000].apply(lambda x: test3(x['id1'], x['id2']), axis=1)\n",
    "test3_b = df_stream[1000000:2000000].apply(lambda x: test3(x['id1'], x['id2']), axis=1)\n",
    "test3_c = df_stream[2000000:].apply(lambda x: test3(x['id1'], x['id2']), axis=1)\n",
    "df_stream['test3'] = pd.concat([test3_a, test3_b, test3_c])\n",
    "\n",
    "print('Test 3 results:')\n",
    "print(df_stream['test3'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'paymo_output/output1.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-4cc098696dd4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mfile_3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'output3.txt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mdf_stream\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mdf_stream\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mdf_stream\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test3'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python3.5/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mto_csv\u001b[0;34m(self, path, index, sep, na_rep, float_format, header, index_label, mode, encoding, date_format, decimal)\u001b[0m\n\u001b[1;32m   2619\u001b[0m                            \u001b[0mindex_label\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindex_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2620\u001b[0m                            \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdate_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdate_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2621\u001b[0;31m                            decimal=decimal)\n\u001b[0m\u001b[1;32m   2622\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2623\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python3.5/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mto_csv\u001b[0;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, tupleize_cols, date_format, doublequote, escapechar, decimal)\u001b[0m\n\u001b[1;32m   1379\u001b[0m                                      \u001b[0mdoublequote\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdoublequote\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1380\u001b[0m                                      escapechar=escapechar, decimal=decimal)\n\u001b[0;32m-> 1381\u001b[0;31m         \u001b[0mformatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1382\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1383\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpath_or_buf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python3.5/site-packages/pandas/formats/format.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1458\u001b[0m             f = _get_handle(self.path_or_buf, self.mode,\n\u001b[1;32m   1459\u001b[0m                             \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1460\u001b[0;31m                             compression=self.compression)\n\u001b[0m\u001b[1;32m   1461\u001b[0m             \u001b[0mclose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1462\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python3.5/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36m_get_handle\u001b[0;34m(path, mode, encoding, compression, memory_map)\u001b[0m\n\u001b[1;32m    330\u001b[0m                 \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m                 \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'replace'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'paymo_output/output1.txt'"
     ]
    }
   ],
   "source": [
    "## OUTPUT TO TEXT FILE ## \n",
    "input_dir = 'paymo_output/'\n",
    "file_1 = input_dir + 'output1.txt'\n",
    "file_2 = input_dir + 'output2.txt'\n",
    "file_3 = input_dir + 'output3.txt'\n",
    "\n",
    "df_stream['test1'].to_csv(file_1, index=False)\n",
    "df_stream['test2'].to_csv(file_2, index=False)\n",
    "df_stream['test3'].to_csv(file_3, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
