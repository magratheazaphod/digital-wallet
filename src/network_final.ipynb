{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Production version of code for Insight Data Engineers program.\n",
    "\n",
    "# Overall strategy - define module \"account\" which contains a list of \"contacts\"\n",
    "# Contacts are saved as Python dictionary, using account number as key and the \"tier\" level as value\n",
    "# \"tier\" being the shorthand for the number of degrees of separation.\n",
    "# Since Python dictionaries are extremely fast (equivalent to hash tables), should be able to flag transaction fast\n",
    "\n",
    "import pandas as pd\n",
    "import csv\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Paths to data files\n",
    "input_dir = '../paymo_input/'\n",
    "batch_file = 'batch_payment.csv'\n",
    "stream_file = 'stream_payment.csv'\n",
    "batch_path = input_dir + batch_file\n",
    "stream_path = input_dir + stream_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Difficult to parse messages section because it's possible for it to use commas\n",
    "## Below, figured out solution - import raw string, then use regular expressions to pick out each component\n",
    "## we choose everything after the fourth comma to be the message\n",
    "batch_raw = []\n",
    "\n",
    "with open(batch_path, newline='') as csvfile:\n",
    "    for line in csvfile:\n",
    "        batch_raw.append(line.strip())\n",
    "        \n",
    "df_batch = pd.DataFrame(batch_raw, columns = ['raw'])\n",
    "\n",
    "## USE REGEX to parse out - everything before first comma is time, second comma is id1 and so on\n",
    "## able to pick up messages with commas in them since it picks up all leftover characters at end of line\n",
    "df_batch['time'] = df_batch['raw'].str.extract('^(.+?),', expand=True)\n",
    "df_batch['id1'] = df_batch['raw'].str.extract('^[^,]*,([^,]*),', expand=True)\n",
    "df_batch['id2'] = df_batch['raw'].str.extract('^[^,]*,[^,]*,([^,]*),', expand=True)\n",
    "df_batch['amount'] = df_batch['raw'].str.extract('^[^,]*,[^,]*,[^,]*,([^,]*),', expand=True)\n",
    "df_batch['message'] = df_batch['raw'].str.extract('^[^,]*,[^,]*,[^,]*,[^,]*,(.*$)', expand=True)\n",
    "\n",
    "#can get rid of raw data now that we've parsed everything\n",
    "df_batch = df_batch.drop('raw', 1)\n",
    "df_batch = df_batch.drop(0, 0) #drop header row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#for convenience, let's call the id1 users 'givers' and id2 users 'receivers'\n",
    "#strategy is to use the pandas groupby command to obtain list of all partners in transactions where <user_id> is giver\n",
    "#repeat for transactions where <user_id> is receiver, then combine into single list\n",
    "givers = df_batch.groupby('id1')\n",
    "receivers = df_batch.groupby('id2')\n",
    "partners_1 = {}\n",
    "partners_2 = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping invalid key:  no. Even if the union were a matter of economic indifference\n",
      "Skipping invalid key:  and even if it were to be disadvantageous from the economic standpoint\n"
     ]
    }
   ],
   "source": [
    "## find all transactions where user <user_id> was giver, then find list of partners in those transactions\n",
    "for user_id,transactions in givers:\n",
    "    \n",
    "    #store list of all transaction partners as list (easiest type to extend later)\n",
    "    try:\n",
    "        partners_1[int(user_id)] = list(givers.get_group(user_id)['id2'].astype(int))\n",
    "   \n",
    "    #some lines of batch_payment.txt and stream_payment.txt are off - omit malformed entries\n",
    "    except (KeyError, ValueError) as BadLine:\n",
    "        print(\"Skipping invalid key:\",user_id)\n",
    "    \n",
    "## same as before for all transactions where <user_id> was receiver\n",
    "for user_id,transactions in receivers:\n",
    "    \n",
    "    #store list of all transaction partners as list (easiest type to extend later)\n",
    "    try: \n",
    "        partners_2[int(user_id)] = list(receivers.get_group(user_id)['id1'].astype(int))\n",
    "        \n",
    "    #some lines of batch_payment.txt and stream_payment.txt are off - omit malformed entries\n",
    "    except (KeyError, ValueError) as BadLine:\n",
    "        print(\"Skipping invalid key:\",user_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## it's possible that some users only show up as givers and others only as receivers - combine to master list of all IDs\n",
    "## in actuality for the provided batch_payment.txt all users show up as givers at least once, but not safe to assume\n",
    "user_list_1 = np.array(list(partners_1.keys()))\n",
    "user_list_2 = np.array(list(partners_2.keys()))\n",
    "user_list = np.unique(np.concatenate([user_list_1,user_list_2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (0,) (242,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-87ac319b54c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0muser_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpartners_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mpp\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mpartners_1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0muser_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0muser_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpartners_2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (0,) (242,) "
     ]
    }
   ],
   "source": [
    "user_master_list = {}\n",
    "\n",
    "#cycle through all users and agglomerate partners from all transactions\n",
    "#conversion back and forth between list and numpy array is pretty fast\n",
    "#lists easier to append to, hence why stored as list, but also wanted to use numpy.unique function.\n",
    "for user_id in user_list:\n",
    "    \n",
    "    pp = []\n",
    "    \n",
    "    if user_id in partners_1.keys():\n",
    "        pp += partners_1[user_id]\n",
    "        \n",
    "    if user_id in partners_2.keys():\n",
    "        pp += partners_2[user_id]\n",
    "        \n",
    "    #reduce to (sorted) list of all unique partners\n",
    "    user_master_list[user_id] = User(user_id, list(np.unique(pp)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## use the find_new_friends function (stored in user_class.py) to supplement friend tiers down to level of interest\n",
    "#this is the most labor-intensive part of the program, unsurprisingly - n=2 takes 2 minutes on my macbook pro\n",
    "\n",
    "#unfortunately, n=3 and n=4 are highly memory intensive...couldn't get them to work consistently\n",
    "tier_depth = 2\n",
    "\n",
    "#successively add tiers of friendship to every user in user_master_list\n",
    "for tier in range(2,tier_depth+1):\n",
    "    \n",
    "    print(\"Building lists of connections of degree\", tier, \"for each user...\")\n",
    "    for user_id, user_info in user_master_list.items():\n",
    "        user_info.friends[tier] = find_new_friends(user_master_list,user_info,tier)\n",
    "        \n",
    "print(\"Done. Connections of degree n accessible via User.friends[n]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## create forward lookup ##\n",
    "## for each user, dictionary dos links a user id with the level of friendship\n",
    "for user_id, user_data in user_master_list.items():\n",
    "    user_data.build_dos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## DEFINE TESTS ##\n",
    "\n",
    "## the following tests take in pairs of user IDs as strings, convert them to int and use\n",
    "## User.dos to look up whether they are connected to each other, and if so at what degree.\n",
    "\n",
    "#first test - have these people had a transaction with each other in the batch data set?\n",
    "def test1(id1,id2):\n",
    "    \n",
    "    if type(id1)==str and type(id2)==str:\n",
    "    \n",
    "        #converts user IDs from string into ints, which we use as dictionary keys\n",
    "        try:\n",
    "            user1 = int(id1)\n",
    "        except ValueError:\n",
    "            return 'unverified'\n",
    "\n",
    "        try:\n",
    "            user2 = int(id2)\n",
    "        except ValueError:\n",
    "            return 'unverified'\n",
    "        \n",
    "        #this being python, the following line doesn't take up new memory - just a shorthand\n",
    "        if user1 in user_master_list.keys(): #have to check in case id1 is a new user\n",
    "            user_dos = user_master_list[user1].dos\n",
    "\n",
    "            if user2 in user_dos.keys():\n",
    "                if user_dos[user2] == 1:\n",
    "                    return 'trusted'\n",
    "    \n",
    "    return 'unverified'\n",
    "\n",
    "#second test - is the transaction partner either a 1st or 2nd degree connection?    \n",
    "def test2(id1,id2):\n",
    "    \n",
    "    if type(id1)==str and type(id2)==str:\n",
    "    \n",
    "        #converts user IDs from string into ints, which we use as dictionary keys\n",
    "        try:\n",
    "            user1 = int(id1)\n",
    "        except ValueError:\n",
    "            return 'unverified'\n",
    "\n",
    "        try:\n",
    "            user2 = int(id2)\n",
    "        except ValueError:\n",
    "            return 'unverified'\n",
    "    \n",
    "        #this being python, the following line doesn't take up new memory - just a shorthand\n",
    "        if user1 in user_master_list.keys(): #have to check in case id1 is a new user\n",
    "\n",
    "            user_dos = user_master_list[user1].dos\n",
    "\n",
    "            if user2 in user_dos.keys():\n",
    "                if user_dos[user2] in range (1,3):\n",
    "                    return 'trusted'\n",
    "    \n",
    "    return 'unverified'\n",
    "    \n",
    "#third test - is the transaction partner at least a 4th degree connection?    \n",
    "def test3(id1,id2):\n",
    "    \n",
    "    if type(id1)==str and type(id2)==str:\n",
    "    \n",
    "        #converts user IDs from string into ints, which we use as dictionary keys\n",
    "        try:\n",
    "            user1 = int(id1)\n",
    "        except ValueError:\n",
    "            return 'unverified'\n",
    "\n",
    "        try:\n",
    "            user2 = int(id2)\n",
    "        except ValueError:\n",
    "            return 'unverified'\n",
    "        \n",
    "        #this being python, the following line doesn't take up new memory - just a shorthand\n",
    "        if user1 in user_master_list.keys(): #have to check in case id1 is a new user    \n",
    "\n",
    "            user_dos = user_master_list[user1].dos\n",
    "\n",
    "            if user2 in user_dos.keys():\n",
    "                if user_dos[user2] in range(1,5):\n",
    "                    return 'trusted'\n",
    "\n",
    "    return 'unverified'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stream_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-ffe2793db0d0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mstream_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'message'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstream_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcsvfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mreader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDictReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsvfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'stream_path' is not defined"
     ]
    }
   ],
   "source": [
    "## again, using regular expressions to parse from raw data.\n",
    "stream_raw = []\n",
    "\n",
    "with open(stream_path, newline='') as csvfile:\n",
    "    for line in csvfile:\n",
    "        stream_raw.append(line.strip())\n",
    "        \n",
    "df_stream = pd.DataFrame(stream_raw, columns = ['raw'])\n",
    "\n",
    "## USE REGEX to parse out - everything before first comma is time, second comma is id1 and so on\n",
    "## able to pick up messages with commas in them since it picks up all leftover characters at end of line\n",
    "df_stream['time'] = df_stream['raw'].str.extract('^(.+?),', expand=True)\n",
    "df_stream['id1'] = df_stream['raw'].str.extract('^[^,]*,([^,]*),', expand=True)\n",
    "df_stream['id2'] = df_stream['raw'].str.extract('^[^,]*,[^,]*,([^,]*),', expand=True)\n",
    "df_stream['amount'] = df_stream['raw'].str.extract('^[^,]*,[^,]*,[^,]*,([^,]*),', expand=True)\n",
    "df_stream['message'] = df_stream['raw'].str.extract('^[^,]*,[^,]*,[^,]*,[^,]*,(.*$)', expand=True)\n",
    "\n",
    "#can get rid of raw data now that we've parsed everything\n",
    "df_stream = df_stream.drop('raw', 1)\n",
    "df_stream = df_stream.drop(0, 0) #drop header row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_stream' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-5a3fa3397ce4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#test 1 - immediate adjacency\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtest1_a\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_stream\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1000000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtest1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mtest1_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_stream\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1000000\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2000000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtest1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mtest1_c\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_stream\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2000000\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtest1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_stream' is not defined"
     ]
    }
   ],
   "source": [
    "## create output columns\n",
    "## I found that pandas started to slow down drastically when trying to calculate over 1,000,000\n",
    "## entries at a time...hence broken into pieces\n",
    "\n",
    "#test 1 - immediate adjacency\n",
    "test1_a = df_stream[:1000000].apply(lambda x: test1(x['id1'], x['id2']), axis=1)\n",
    "test1_b = df_stream[1000000:2000000].apply(lambda x: test1(x['id1'], x['id2']), axis=1)\n",
    "test1_c = df_stream[2000000:].apply(lambda x: test1(x['id1'], x['id2']), axis=1)\n",
    "df_stream['test1'] = pd.concat([test1_a, test1_b, test1_c])\n",
    "\n",
    "print('Test 1 results:')\n",
    "print(df_stream['test1'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#test 2 - 2nd degree friends pass as verified\n",
    "test2_a = df_stream[:1000000].apply(lambda x: test2(x['id1'], x['id2']), axis=1)\n",
    "test2_b = df_stream[1000000:2000000].apply(lambda x: test2(x['id1'], x['id2']), axis=1)\n",
    "test2_c = df_stream[2000000:].apply(lambda x: test2(x['id1'], x['id2']), axis=1)\n",
    "df_stream['test2'] = pd.concat([test2_a, test2_b, test2_c])\n",
    "\n",
    "print('Test 2 results:')\n",
    "print(df_stream['test2'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#test 3\n",
    "test3_a = df_stream[:1000000].apply(lambda x: test3(x['id1'], x['id2']), axis=1)\n",
    "test3_b = df_stream[1000000:2000000].apply(lambda x: test3(x['id1'], x['id2']), axis=1)\n",
    "test3_c = df_stream[2000000:].apply(lambda x: test3(x['id1'], x['id2']), axis=1)\n",
    "df_stream['test3'] = pd.concat([test3_a, test3_b, test3_c])\n",
    "\n",
    "print('Test 3 results:')\n",
    "print(df_stream['test3'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## OUTPUT TO TEXT FILE ## \n",
    "input_dir = 'paymo_output/'\n",
    "file_1 = input_dir + 'output1.txt'\n",
    "file_2 = input_dir + 'output2.txt'\n",
    "file_3 = input_dir + 'output3.txt'\n",
    "\n",
    "df_stream['test1'].to_csv(file_1, index=False)\n",
    "df_stream['test2'].to_csv(file_2, index=False)\n",
    "df_stream['test3'].to_csv(file_3, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
